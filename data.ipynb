{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disney Character Popularity Analysis Notebook\n",
    "# Requirements (pip):\n",
    "# pip install pandas numpy scikit-learn matplotlib seaborn kaggle joblib\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- (OPTIONAL) Download from Kaggle ----------\n",
    "# If you want the notebook to download the dataset via Kaggle API:\n",
    "# 1) Install kaggle (pip install kaggle)\n",
    "# 2) Create ~/.kaggle/kaggle.json with your API token\n",
    "# 3) Uncomment the following lines to download\n",
    "#\n",
    "# import subprocess\n",
    "# datasets = [\n",
    "#     \"thedevastator/disney-character-success-a-comprehensive-analysi\",\n",
    "#     \"suvroo/disney-movies-dataset\"\n",
    "# ]\n",
    "# for d in datasets:\n",
    "#     subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", d, \"-p\", str(DATA_DIR)], check=True)\n",
    "#     # unzip\n",
    "#     zips = list(DATA_DIR.glob(\"*.zip\"))\n",
    "#     for z in zips:\n",
    "#         with zipfile.ZipFile(z, \"r\") as zf:\n",
    "#             zf.extractall(DATA_DIR)\n",
    "#         z.unlink()\n",
    "\n",
    "# NOTE: If you already have the CSVs put them in ./data/ and skip Kaggle download above.\n",
    "\n",
    "# ---------- 1) Load CSVs ----------\n",
    "# These filenames are those commonly contained in the Character Success dataset.\n",
    "# If your filenames differ, adjust the paths below.\n",
    "\n",
    "chars_path = DATA_DIR / \"disney-characters.csv\"            # from thedevastator dataset\n",
    "movies_gross_path = DATA_DIR / \"disney_movies_total_gross.csv\"\n",
    "movies_all_path = DATA_DIR / \"disney_movies_total_gross.csv\"  # fallback to same if only one exists\n",
    "\n",
    "# Try a few likely filenames (some Kaggle dumps use slightly different names)\n",
    "candidates = list(DATA_DIR.glob(\"**/*characters*.csv\")) + list(DATA_DIR.glob(\"**/*character*.csv\"))\n",
    "if candidates:\n",
    "    chars_path = candidates[0]\n",
    "movies_candidates = list(DATA_DIR.glob(\"**/*gross*.csv\")) + list(DATA_DIR.glob(\"**/*movies*.csv\"))\n",
    "if movies_candidates:\n",
    "    movies_gross_path = movies_candidates[0]\n",
    "\n",
    "print(\"Using character file:\", chars_path)\n",
    "print(\"Using movie gross file:\", movies_gross_path)\n",
    "\n",
    "df_chars = pd.read_csv(chars_path)\n",
    "df_movies = pd.read_csv(movies_gross_path)\n",
    "\n",
    "print(\"chars shape:\", df_chars.shape)\n",
    "print(\"movies shape:\", df_movies.shape)\n",
    "\n",
    "# ---------- Quick look ----------\n",
    "display(df_chars.head())\n",
    "display(df_movies.head())\n",
    "\n",
    "# ---------- 2) Merge & initial cleaning ----------\n",
    "# We want: a per-movie row containing movie metadata + main_character + villain + songs + gross\n",
    "# Identify the relevant columns present in the files and standardize them.\n",
    "\n",
    "# Inspect columns\n",
    "print(\"chars columns:\", df_chars.columns.tolist())\n",
    "print(\"movies columns:\", df_movies.columns.tolist())\n",
    "\n",
    "# Common column names: 'movie_title', 'title', 'release_date', 'total_gross', 'inflation_adjusted_gross'\n",
    "# Normalize names:\n",
    "if \"movie_title\" in df_chars.columns:\n",
    "    df_chars = df_chars.rename(columns={\"movie_title\": \"title\"})\n",
    "if \"title\" not in df_chars.columns and \"movie\" in df_chars.columns:\n",
    "    df_chars = df_chars.rename(columns={\"movie\": \"title\"})\n",
    "\n",
    "# For movies df\n",
    "if \"movie\" in df_movies.columns and \"title\" not in df_movies.columns:\n",
    "    df_movies = df_movies.rename(columns={\"movie\": \"title\"})\n",
    "if \"total_gross\" not in df_movies.columns and \"total_gross\" in df_movies.columns:\n",
    "    pass  # keep as is\n",
    "\n",
    "# Merge on title (use left join of characters into movies)\n",
    "# But first, ensure title string normalization\n",
    "def norm_title(s):\n",
    "    if pd.isna(s): return s\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "df_chars['title_norm'] = df_chars['title'].astype(str).apply(norm_title)\n",
    "df_movies['title_norm'] = df_movies['title'].astype(str).apply(norm_title)\n",
    "\n",
    "# There may be duplicates if dataset has one row per character — reduce to main character rows\n",
    "# The chars file often has 'main_character' column; if not, we try to infer by role.\n",
    "if 'main_character' in df_chars.columns:\n",
    "    df_main = df_chars.copy()\n",
    "else:\n",
    "    # fallback: try to filter by role indicator or first listed character per title\n",
    "    df_main = df_chars.sort_values(by=['title_norm']).groupby('title_norm').first().reset_index()\n",
    "\n",
    "# Merge: movie metadata + main-character-level info\n",
    "df = pd.merge(df_movies, df_main, on='title_norm', how='left', suffixes=('_movie','_char'))\n",
    "print(\"Merged DF shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# ---------- 3) Create target: 'popular' ----------\n",
    "# We'll make a binary target: popular vs not popular based on inflation_adjusted_gross.\n",
    "# If inflation_adjusted_gross not present, use total_gross.\n",
    "\n",
    "if 'inflation_adjusted_gross' in df.columns:\n",
    "    gross_col = 'inflation_adjusted_gross'\n",
    "elif 'adjusted_gross' in df.columns:\n",
    "    gross_col = 'adjusted_gross'\n",
    "elif 'total_gross' in df.columns:\n",
    "    gross_col = 'total_gross'\n",
    "else:\n",
    "    # fallback: try to find any column with 'gross' in name\n",
    "    gross_cols = [c for c in df.columns if 'gross' in c.lower()]\n",
    "    gross_col = gross_cols[0] if gross_cols else None\n",
    "\n",
    "print(\"Chosen gross column:\", gross_col)\n",
    "df[gross_col] = pd.to_numeric(df[gross_col], errors='coerce')\n",
    "\n",
    "# Define popularity threshold: top 33% as 'popular'\n",
    "df['popular'] = (df[gross_col] >= df[gross_col].quantile(0.67)).astype(int)\n",
    "print(df['popular'].value_counts())\n",
    "\n",
    "# ---------- 4) Feature engineering ----------\n",
    "# Basic, interpretable features from the columns commonly present.\n",
    "# - release_year, release_month\n",
    "# - has_villain (boolean if villain name present)\n",
    "# - songs_count (count of songs listed)\n",
    "# - title_length (number of words in title)\n",
    "# - main_character_name_length\n",
    "# - main_character_freq (how many movies include same main character in dataset) — a proxy for franchise strength\n",
    "\n",
    "def safe_len_str(s):\n",
    "    if pd.isna(s): return 0\n",
    "    return len(str(s).split())\n",
    "\n",
    "# release date -> year/month\n",
    "if 'release_date' in df.columns:\n",
    "    # try to parse\n",
    "    df['release_date_parsed'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "    df['release_year'] = df['release_date_parsed'].dt.year\n",
    "    df['release_month'] = df['release_date_parsed'].dt.month\n",
    "else:\n",
    "    # try 'year' column\n",
    "    if 'year' in df.columns:\n",
    "        df['release_year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df['release_month'] = np.nan\n",
    "    else:\n",
    "        df['release_year'] = np.nan\n",
    "        df['release_month'] = np.nan\n",
    "\n",
    "df['has_villain'] = (~df['villain'].isna()) & (df['villain'].astype(str).str.strip() != \"\")\n",
    "df['songs_count'] = df['songs'].fillna(\"\").apply(lambda s: len([x for x in str(s).split(';') if x.strip()!=\"\"]))\n",
    "df['title_len_words'] = df['title_movie'].fillna(df['title']).astype(str).apply(safe_len_str)\n",
    "# main character name length\n",
    "name_col = 'main_character' if 'main_character' in df.columns else 'character' if 'character' in df.columns else None\n",
    "if name_col:\n",
    "    df['main_char_name_len'] = df[name_col].fillna(\"\").astype(str).apply(lambda s: len(s))\n",
    "    # frequency of main_character in dataset\n",
    "    df['main_char_freq'] = df[name_col].map(df[name_col].value_counts()).fillna(0)\n",
    "else:\n",
    "    df['main_char_name_len'] = 0\n",
    "    df['main_char_freq'] = 0\n",
    "\n",
    "# Genre / mpaa rating from movies df if present\n",
    "genre_col = None\n",
    "possible_genres = [c for c in df.columns if 'genre' in c.lower()]\n",
    "if possible_genres:\n",
    "    genre_col = possible_genres[0]\n",
    "    # some rows have multiple genres -> keep first genre\n",
    "    df['genre_primary'] = df[genre_col].fillna(\"\").astype(str).apply(lambda s: s.split(',')[0].strip() if s else 'unknown')\n",
    "else:\n",
    "    df['genre_primary'] = 'unknown'\n",
    "\n",
    "# Keep a working features DataFrame\n",
    "features = [\n",
    "    'release_year', 'release_month', 'has_villain', 'songs_count',\n",
    "    'title_len_words', 'main_char_name_len', 'main_char_freq', 'genre_primary'\n",
    "]\n",
    "df_model = df[features + ['popular', gross_col]].copy()\n",
    "print(\"Prepared model df shape:\", df_model.shape)\n",
    "display(df_model.head())\n",
    "\n",
    "# ---------- 5) Split: time-based train / val / test ----------\n",
    "# We'll train on movies released <= 2010, validate on 2011-2013, test on >=2014.\n",
    "# This simulates predicting newer movies from historical patterns.\n",
    "\n",
    "# If release_year is missing for many rows, fallback to random stratified split.\n",
    "if df_model['release_year'].isna().mean() < 0.5:\n",
    "    train_df = df_model[df_model['release_year'] <= 2010].copy()\n",
    "    val_df   = df_model[(df_model['release_year'] > 2010) & (df_model['release_year'] <= 2013)].copy()\n",
    "    test_df  = df_model[df_model['release_year'] >= 2014].copy()\n",
    "    # If any set empty (small dataset) fallback to stratified split\n",
    "    if len(train_df) < 10 or len(val_df) < 5:\n",
    "        print(\"Time-split produced too-small sets, falling back to stratified split.\")\n",
    "        X = df_model.drop(columns=['popular', gross_col])\n",
    "        y = df_model['popular']\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "        # Recreate dataframes\n",
    "        train_df = X_train.copy(); train_df['popular'] = y_train\n",
    "        val_df = X_val.copy(); val_df['popular'] = y_val\n",
    "        test_df = X_test.copy(); test_df['popular'] = y_test\n",
    "else:\n",
    "    # too many missing release_years -> use stratified split\n",
    "    X = df_model.drop(columns=['popular', gross_col])\n",
    "    y = df_model['popular']\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "    train_df = X_train.copy(); train_df['popular'] = y_train\n",
    "    val_df = X_val.copy(); val_df['popular'] = y_val\n",
    "    test_df = X_test.copy(); test_df['popular'] = y_test\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n",
    "print(\"Train popular distribution:\\n\", train_df['popular'].value_counts(normalize=True))\n",
    "\n",
    "# ---------- 6) Preprocessing pipelines ----------\n",
    "# Numeric features:\n",
    "num_features = ['release_year', 'release_month', 'songs_count', 'title_len_words', 'main_char_name_len', 'main_char_freq']\n",
    "cat_features = ['genre_primary', 'has_villain']  # treat has_villain as categorical\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_features),\n",
    "    ('cat', categorical_transformer, cat_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# ---------- 7) Model pipeline ----------\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Separate X/y\n",
    "X_train = train_df.drop(columns=['popular', gross_col], errors='ignore')\n",
    "y_train = train_df['popular']\n",
    "X_val = val_df.drop(columns=['popular', gross_col], errors='ignore')\n",
    "y_val = val_df['popular']\n",
    "\n",
    "# Fit baseline model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# ---------- 8) Validation evaluation ----------\n",
    "y_val_pred = pipe.predict(X_val)\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation F1 (macro):\", f1_score(y_val, y_val_pred, average='macro'))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['not_popular','popular'], yticklabels=['not_popular','popular'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importances (map back to feature names)\n",
    "# Need to extract feature names from preprocessor\n",
    "ohe = pipe.named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']\n",
    "ohe_features = ohe.get_feature_names_out(cat_features)\n",
    "num_features_out = num_features\n",
    "all_features = list(num_features_out) + list(ohe_features)\n",
    "importances = pipe.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=all_features).sort_values(ascending=False)\n",
    "print(\"Top features by importance:\\n\", feat_imp.head(15))\n",
    "\n",
    "# ---------- 9) Quick EDA plots (popularity vs features) ----------\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x='popular', y='songs_count', data=pd.concat([train_df, val_df]))\n",
    "plt.title(\"Songs count vs Popularity\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x='genre_primary', hue='popular', data=pd.concat([train_df, val_df]))\n",
    "plt.title(\"Genre vs Popularity (train+val)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ---------- 10) Save the pipeline ----------\n",
    "joblib.dump(pipe, \"disney_popularity_pipeline.joblib\")\n",
    "print(\"Saved pipeline to disney_popularity_pipeline.joblib\")\n",
    "\n",
    "# You're now ready to run final test evaluation on test_df if you wish. The notebook stops at validation per request.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
